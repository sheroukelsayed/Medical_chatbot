# -*- coding: utf-8 -*-
"""Medical_Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17G35-JjE64BbbZRoyo9_LwpV3lMYZY15
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/QA/

!pip install transformers[torch]
!pip install datasets
!pip install pandas
import pandas as pd
import numpy as np
import nltk

!pip install --upgrade simplet5
from simplet5 import SimpleT5
!pip install rouge-score
from rouge_score import rouge_scorer

train_data=pd.read_csv('/content/drive/My Drive/QA/train.csv')
test_data=pd.read_csv('/content/drive/My Drive/QA/MedInfo2019-QA-Medications.csv')


test_data["Question"] = test_data["Question"].astype(str)
test_data["Answer"]= test_data["Answer"].astype(str)


train_data_subset = train_data[["Question", "Answer"]]
test_data_subset = test_data[["Question", "Answer"]]


merged_df = pd.concat([train_data_subset, test_data_subset], ignore_index=True)


# Step 2: Shuffle the merged DataFrame
shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)

shuffled_df

import pandas as pd
from sklearn.model_selection import train_test_split
# Step 3: Split into training and testing sets using train_test_split
train_df, test_df = train_test_split(shuffled_df, test_size=0.3, random_state=42)



model = SimpleT5()
model.from_pretrained(model_type="t5", model_name="t5-base")

q_list = "question: " + train_df['Question']                          # questions list to feed the model
n_list ="Answer: "+ train_df['Answer']   # answers list to feed the model

dict_data = {'source_text': q_list,
      'target_text': n_list}

df_train = pd.DataFrame(dict_data)
df_train.head()

q_list = "question: " + test_df['Question']                          # questions list to feed the model
n_list ="Answer: "+ test_df['Answer']   # answers list to feed the model

dict_data_test = {'source_text': q_list,
      'target_text': n_list}

df_test = pd.DataFrame(dict_data_test)
df_test.head()

# let's load the trained model from the local output folder for inferencing:
model.load_model("/content/drive/MyDrive/QA/outputs/simplet5-epoch-4-train-loss-1.3677-val-loss-1.3708")

model.train(train_df =df_train,
            eval_df = df_test,
            source_max_token_len=128,
            target_max_token_len=128,
            batch_size=16, max_epochs=5)



Question="what do you know about heart diseases?"

Answer=model.predict(Question)
print(Answer)

predictions = model.predict(df_test)



scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

for actual, predicted in zip(df_train['target_text'], predictions):
    scores = scorer.score(actual, predicted)
    print(f"Rouge-1: {scores['rouge1'].fmeasure}, Rouge-2: {scores['rouge2'].fmeasure}, Rouge-L: {scores['rougeL'].fmeasure}")

from nltk.translate.bleu_score import sentence_bleu

# Assuming actual_answers and predictions are lists of tokenized sentences
bleu_scores = [sentence_bleu([actual.split()], predicted.split()) for actual, predicted in zip(df_train["target_text"], predictions)]
print(f"Average BLEU Score: {bleu_scores}")
average_bleu = sum(bleu_scores) / len(bleu_scores)

print(f"Average BLEU Score: {average_bleu}")

def generate_response(user_input):
    input_text = f"question: {user_input} context: medical information"

    output_ids = model.predict(input_text )

    return response




from flask import Flask, render_template, request, jsonify


app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/get_response', methods=['POST'])
def get_response():
    user_input = request.form['user_input']
    response = generate_response(user_input)
    return jsonify({'response': response})

if __name__ == '__main__':
    app.run(debug=True)